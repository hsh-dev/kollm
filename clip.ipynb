{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clip"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pip install flax==0.6.10\n",
    "pip install transformers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Define Model \n",
    "(feat. https://github.com/jaketae/koclip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "from transformers.configuration_utils import PretrainedConfig\n",
    "from transformers.utils import logging\n",
    "\n",
    "# logger = logging.get_logger(__name__)\n",
    "\n",
    "\n",
    "class HybridCLIPConfig(PretrainedConfig):\n",
    "    r\"\"\"\n",
    "    :class:`HybridCLIPConfig` is the configuration class to store the configuration of a\n",
    "    :class:`~HybridCLIPModel`. It is used to instantiate HybridCLIPModel model according to the specified arguments,\n",
    "    defining the text model and vision model configs.\n",
    "    Configuration objects inherit from :class:`~transformers.PretrainedConfig` and can be used to control the model\n",
    "    outputs. Read the documentation from :class:`~transformers.PretrainedConfig` for more information.\n",
    "    Args:\n",
    "        text_config_dict (:obj:`dict`):\n",
    "            Dictionary of configuration options that defines text model config.\n",
    "        vision_config_dict (:obj:`dict`):\n",
    "            Dictionary of configuration options that defines vison model config.\n",
    "        projection_dim (:obj:`int`, `optional`, defaults to 512):\n",
    "            Dimentionality of text and vision projection layers.\n",
    "        kwargs (`optional`):\n",
    "            Dictionary of keyword arguments.\n",
    "    Examples::\n",
    "        >>> from transformers import BertConfig, CLIPConfig, HybridCLIPConfig, FlaxHybridCLIP\n",
    "        >>> # Initializing a BERT and CLIP configuration\n",
    "        >>> config_text = BertConfig()\n",
    "        >>> config_vision = CLIPConfig()\n",
    "        >>> config = HybridCLIPConfig.from_text_vision_configs(config_text, config_vision, projection_dim=512)\n",
    "        >>> # Initializing a BERT and CLIPVision model\n",
    "        >>> model = EncoderDecoderModel(config=config)\n",
    "        >>> # Accessing the model configuration\n",
    "        >>> config_text = model.config.text_config\n",
    "        >>> config_vision  = model.config.vision_config\n",
    "        >>> # Saving the model, including its configuration\n",
    "        >>> model.save_pretrained('my-model')\n",
    "        >>> # loading model and config from pretrained folder\n",
    "        >>> encoder_decoder_config = HybridCLIPConfig.from_pretrained('my-model')\n",
    "        >>> model = FlaxHybridCLIP.from_pretrained('my-model', config=encoder_decoder_config)\n",
    "    \"\"\"\n",
    "\n",
    "    model_type = \"hybrid-clip\"\n",
    "    is_composition = True\n",
    "\n",
    "    def __init__(self, projection_dim=512, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        if \"text_config\" not in kwargs:\n",
    "            raise ValueError(\"`text_config` can not be `None`.\")\n",
    "\n",
    "        if \"vision_config\" not in kwargs:\n",
    "            raise ValueError(\"`vision_config` can not be `None`.\")\n",
    "\n",
    "        text_config = kwargs.pop(\"text_config\")\n",
    "        vision_config = kwargs.pop(\"vision_config\")\n",
    "\n",
    "        text_model_type = text_config.pop(\"model_type\")\n",
    "        vision_model_type = vision_config.pop(\"model_type\")\n",
    "\n",
    "        from transformers import AutoConfig\n",
    "\n",
    "        self.text_config = AutoConfig.for_model(text_model_type, **text_config)\n",
    "\n",
    "        if vision_model_type == \"clip\":\n",
    "            self.vision_config = AutoConfig.for_model(\n",
    "                vision_model_type, **vision_config\n",
    "            ).vision_config\n",
    "        elif vision_model_type == \"clip_vision_model\":\n",
    "            from transformers import CLIPVisionConfig\n",
    "\n",
    "            self.vision_config = CLIPVisionConfig(**vision_config)\n",
    "        else:\n",
    "            self.vision_config = AutoConfig.for_model(\n",
    "                vision_model_type, **vision_config\n",
    "            )\n",
    "\n",
    "        self.projection_dim = projection_dim\n",
    "        self.initializer_factor = 1.0\n",
    "\n",
    "    @classmethod\n",
    "    def from_text_vision_configs(\n",
    "        cls, text_config: PretrainedConfig, vision_config: PretrainedConfig, **kwargs\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        Instantiate a :class:`HybridCLIPConfig` (or a derived class) from text model configuration and\n",
    "        vision model configuration.\n",
    "        Returns:\n",
    "            :class:`HybridCLIPConfig`: An instance of a configuration object\n",
    "        \"\"\"\n",
    "\n",
    "        return cls(\n",
    "            text_config=text_config.to_dict(),\n",
    "            vision_config=vision_config.to_dict(),\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "    def to_dict(self):\n",
    "        \"\"\"\n",
    "        Serializes this instance to a Python dictionary. Override the default\n",
    "        :meth:`~transformers.PretrainedConfig.to_dict`.\n",
    "        Returns:\n",
    "            :obj:`Dict[str, any]`: Dictionary of all the attributes that make up this configuration instance,\n",
    "        \"\"\"\n",
    "        output = copy.deepcopy(self.__dict__)\n",
    "        output[\"text_config\"] = self.text_config.to_dict()\n",
    "        output[\"vision_config\"] = self.vision_config.to_dict()\n",
    "        output[\"model_type\"] = self.__class__.model_type\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2021 The HuggingFace Team. All rights reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import flax.linen as nn\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax.core.frozen_dict import FrozenDict\n",
    "from transformers import FLAX_MODEL_MAPPING, FlaxCLIPVisionModel\n",
    "from transformers.modeling_flax_utils import FlaxPreTrainedModel\n",
    "from transformers.models.clip.modeling_flax_clip import FlaxCLIPOutput\n",
    "from transformers.utils import logging\n",
    "\n",
    "# from .config import HybridCLIPConfig\n",
    "\n",
    "# logger = logging.get_logger(__name__)\n",
    "\n",
    "\n",
    "class FlaxHybridCLIPModule(nn.Module):\n",
    "    config: HybridCLIPConfig\n",
    "    dtype: jnp.dtype = jnp.float32\n",
    "\n",
    "    def setup(self):\n",
    "        text_config = self.config.text_config\n",
    "        vision_config = self.config.vision_config\n",
    "\n",
    "        self.projection_dim = self.config.projection_dim\n",
    "        self.text_embed_dim = text_config.hidden_size\n",
    "        self.vision_embed_dim = vision_config.hidden_size\n",
    "\n",
    "        text_module = FLAX_MODEL_MAPPING[self.config.text_config.__class__].module_class\n",
    "        vision_module = FLAX_MODEL_MAPPING.get(\n",
    "            self.config.vision_config.__class__, FlaxCLIPVisionModel\n",
    "        ).module_class\n",
    "\n",
    "        self.text_model = text_module(text_config, dtype=self.dtype)\n",
    "        self.vision_model = vision_module(vision_config, dtype=self.dtype)\n",
    "\n",
    "        self.visual_projection = nn.Dense(\n",
    "            self.projection_dim,\n",
    "            dtype=self.dtype,\n",
    "            kernel_init=jax.nn.initializers.normal(0.02, dtype=self.dtype),\n",
    "            use_bias=False,\n",
    "        )\n",
    "        self.text_projection = nn.Dense(\n",
    "            self.projection_dim,\n",
    "            dtype=self.dtype,\n",
    "            kernel_init=jax.nn.initializers.normal(0.02, dtype=self.dtype),\n",
    "            use_bias=False,\n",
    "        )\n",
    "        self.logit_scale = self.param(\"logit_scale\", jax.nn.initializers.ones, [])\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        pixel_values=None,\n",
    "        attention_mask=None,\n",
    "        position_ids=None,\n",
    "        token_type_ids=None,\n",
    "        deterministic: bool = True,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        return_dict = (\n",
    "            return_dict if return_dict is not None else self.config.return_dict\n",
    "        )\n",
    "\n",
    "        vision_outputs = self.vision_model(\n",
    "            pixel_values=pixel_values,\n",
    "            deterministic=deterministic,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        text_outputs = self.text_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            deterministic=deterministic,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        image_embeds = vision_outputs[1]\n",
    "        image_embeds = self.visual_projection(image_embeds)\n",
    "\n",
    "        text_embeds = text_outputs[1]\n",
    "        text_embeds = self.text_projection(text_embeds)\n",
    "\n",
    "        # normalized features\n",
    "        image_embeds = image_embeds / jnp.linalg.norm(\n",
    "            image_embeds, axis=-1, keepdims=True\n",
    "        )\n",
    "        text_embeds = text_embeds / jnp.linalg.norm(text_embeds, axis=-1, keepdims=True)\n",
    "\n",
    "        # cosine similarity as logits\n",
    "        logit_scale = jnp.exp(self.logit_scale)\n",
    "        logits_per_text = jnp.matmul(text_embeds, image_embeds.T) * logit_scale\n",
    "        logits_per_image = logits_per_text.T\n",
    "\n",
    "        if not return_dict:\n",
    "            return (\n",
    "                logits_per_image,\n",
    "                logits_per_text,\n",
    "                text_embeds,\n",
    "                image_embeds,\n",
    "                text_outputs,\n",
    "                vision_outputs,\n",
    "            )\n",
    "\n",
    "        return FlaxCLIPOutput(\n",
    "            logits_per_image=logits_per_image,\n",
    "            logits_per_text=logits_per_text,\n",
    "            text_embeds=text_embeds,\n",
    "            image_embeds=image_embeds,\n",
    "            text_model_output=text_outputs,\n",
    "            vision_model_output=vision_outputs,\n",
    "        )\n",
    "\n",
    "\n",
    "class FlaxHybridCLIP(FlaxPreTrainedModel):\n",
    "    config_class = HybridCLIPConfig\n",
    "    module_class = FlaxHybridCLIPModule\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: HybridCLIPConfig,\n",
    "        input_shape: Optional[Tuple] = None,\n",
    "        seed: int = 0,\n",
    "        dtype: jnp.dtype = jnp.float32,\n",
    "        _do_init=True,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        if input_shape is None:\n",
    "            input_shape = (\n",
    "                (1, 1),\n",
    "                (\n",
    "                    1,\n",
    "                    config.vision_config.image_size,\n",
    "                    config.vision_config.image_size,\n",
    "                    3,\n",
    "                ),\n",
    "            )\n",
    "\n",
    "        module = self.module_class(config=config, dtype=dtype, **kwargs)\n",
    "        super().__init__(\n",
    "            config, module, input_shape=input_shape, seed=seed, dtype=dtype, _do_init=_do_init\n",
    "        )\n",
    "\n",
    "    def init_weights(self, rng: jax.random.PRNGKey, input_shape: Tuple) -> FrozenDict:\n",
    "        # init input tensor\n",
    "        input_ids = jnp.zeros(input_shape[0], dtype=\"i4\")\n",
    "        position_ids = jnp.broadcast_to(\n",
    "            jnp.arange(jnp.atleast_2d(input_ids).shape[-1]), input_shape[0]\n",
    "        )\n",
    "        token_type_ids = jnp.ones_like(input_ids)\n",
    "        attention_mask = jnp.ones_like(input_ids)\n",
    "\n",
    "        pixel_values = jax.random.normal(rng, input_shape[1])\n",
    "\n",
    "        params_rng, dropout_rng = jax.random.split(rng)\n",
    "        rngs = {\"params\": params_rng, \"dropout\": dropout_rng}\n",
    "\n",
    "        return self.module.init(\n",
    "            rngs, input_ids, pixel_values, attention_mask, position_ids, token_type_ids\n",
    "        )[\"params\"]\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        input_ids,\n",
    "        pixel_values,\n",
    "        attention_mask=None,\n",
    "        position_ids=None,\n",
    "        token_type_ids=None,\n",
    "        params: dict = None,\n",
    "        dropout_rng: jax.random.PRNGKey = None,\n",
    "        train: bool = False,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ):\n",
    "        output_attentions = (\n",
    "            output_attentions\n",
    "            if output_attentions is not None\n",
    "            else self.config.output_attentions\n",
    "        )\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states\n",
    "            if output_hidden_states is not None\n",
    "            else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = (\n",
    "            return_dict if return_dict is not None else self.config.return_dict\n",
    "        )\n",
    "\n",
    "        if position_ids is None:\n",
    "            position_ids = jnp.broadcast_to(\n",
    "                jnp.arange(jnp.atleast_2d(input_ids).shape[-1]), input_ids.shape\n",
    "            )\n",
    "\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = jnp.zeros_like(input_ids)\n",
    "\n",
    "        if attention_mask is None:\n",
    "            attention_mask = jnp.ones_like(input_ids)\n",
    "\n",
    "        # Handle any PRNG if needed\n",
    "        rngs = {}\n",
    "        if dropout_rng is not None:\n",
    "            rngs[\"dropout\"] = dropout_rng\n",
    "\n",
    "        return self.module.apply(\n",
    "            {\"params\": params or self.params},\n",
    "            jnp.array(input_ids, dtype=\"i4\"),\n",
    "            jnp.array(pixel_values, dtype=jnp.float32),\n",
    "            jnp.array(attention_mask, dtype=\"i4\"),\n",
    "            jnp.array(position_ids, dtype=\"i4\"),\n",
    "            jnp.array(token_type_ids, dtype=\"i4\"),\n",
    "            not train,\n",
    "            output_attentions,\n",
    "            output_hidden_states,\n",
    "            return_dict,\n",
    "            rngs=rngs,\n",
    "        )\n",
    "\n",
    "    def get_text_features(\n",
    "        self,\n",
    "        input_ids,\n",
    "        attention_mask=None,\n",
    "        position_ids=None,\n",
    "        token_type_ids=None,\n",
    "        dropout_rng: jax.random.PRNGKey = None,\n",
    "        train=False,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        Args:\n",
    "            input_ids (:obj:`numpy.ndarray` of shape :obj:`(batch_size, sequence_length)`):\n",
    "                Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you\n",
    "                provide it.\n",
    "                Indices can be obtained using :class:`~transformers.PreTrainedTokenizer`. See\n",
    "                :meth:`transformers.PreTrainedTokenizer.encode` and :meth:`transformers.PreTrainedTokenizer.__call__`\n",
    "                for details.\n",
    "                `What are input IDs? <../glossary.html#input-ids>`__\n",
    "        Returns:\n",
    "            text_features (:obj:`jax_xla.DeviceArray` of shape :obj:`(batch_size, output_dim`): The text embeddings\n",
    "            obtained by applying the projection layer to the pooled output of text model.\n",
    "        \"\"\"\n",
    "        if position_ids is None:\n",
    "            position_ids = jnp.broadcast_to(\n",
    "                jnp.arange(jnp.atleast_2d(input_ids).shape[-1]), input_ids.shape\n",
    "            )\n",
    "\n",
    "        if token_type_ids is None:\n",
    "            token_type_ids = jnp.zeros_like(input_ids)\n",
    "\n",
    "        if attention_mask is None:\n",
    "            attention_mask = jnp.ones_like(input_ids)\n",
    "\n",
    "        # Handle any PRNG if needed\n",
    "        rngs = {}\n",
    "        if dropout_rng is not None:\n",
    "            rngs[\"dropout\"] = dropout_rng\n",
    "\n",
    "        def _get_features(\n",
    "            module,\n",
    "            input_ids,\n",
    "            attention_mask,\n",
    "            position_ids,\n",
    "            token_type_ids,\n",
    "            deterministic,\n",
    "        ):\n",
    "            text_outputs = module.text_model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                position_ids=position_ids,\n",
    "                token_type_ids=token_type_ids,\n",
    "                deterministic=deterministic,\n",
    "            )\n",
    "            pooled_output = text_outputs[1]\n",
    "            text_features = module.text_projection(pooled_output)\n",
    "            return text_features\n",
    "\n",
    "        return self.module.apply(\n",
    "            {\"params\": self.params},\n",
    "            jnp.array(input_ids, dtype=\"i4\"),\n",
    "            jnp.array(attention_mask, dtype=\"i4\"),\n",
    "            jnp.array(position_ids, dtype=\"i4\"),\n",
    "            jnp.array(token_type_ids, dtype=\"i4\"),\n",
    "            not train,\n",
    "            method=_get_features,\n",
    "            rngs=rngs,\n",
    "        )\n",
    "\n",
    "    def get_image_features(\n",
    "        self, pixel_values, dropout_rng: jax.random.PRNGKey = None, train=False\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        Args:\n",
    "            pixel_values (:obj:`numpy.ndarray` of shape :obj:`(batch_size, num_channels, height, width)`):\n",
    "                Pixel values. Padding will be ignored by default should you provide it. Pixel values can be obtained\n",
    "                using :class:`~transformers.ImageFeatureExtractionMixin`. See\n",
    "                :meth:`transformers.ImageFeatureExtractionMixin.__call__` for details.\n",
    "        Returns:\n",
    "            image_features (:obj:`jax_xla.DeviceArray` of shape :obj:`(batch_size, output_dim`): The image embeddings\n",
    "            obtained by applying the projection layer to the pooled output of vision model.\n",
    "        \"\"\"\n",
    "\n",
    "        # Handle any PRNG if needed\n",
    "        rngs = {}\n",
    "        if dropout_rng is not None:\n",
    "            rngs[\"dropout\"] = dropout_rng\n",
    "\n",
    "        def _get_features(module, pixel_values, deterministic):\n",
    "            vision_outputs = module.vision_model(\n",
    "                pixel_values=pixel_values, deterministic=deterministic\n",
    "            )\n",
    "            pooled_output = vision_outputs[1]  # pooled_output\n",
    "            image_features = module.visual_projection(pooled_output)\n",
    "            return image_features\n",
    "\n",
    "        return self.module.apply(\n",
    "            {\"params\": self.params},\n",
    "            jnp.array(pixel_values, dtype=jnp.float32),\n",
    "            not train,\n",
    "            method=_get_features,\n",
    "            rngs=rngs,\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def from_text_vision_pretrained(\n",
    "        cls,\n",
    "        text_model_name_or_path: str = None,\n",
    "        vision_model_name_or_path: str = None,\n",
    "        *model_args,\n",
    "        **kwargs,\n",
    "    ) -> FlaxPreTrainedModel:\n",
    "        \"\"\"\n",
    "        Params:\n",
    "            text_model_name_or_path (:obj: `str`, `optional`):\n",
    "                Information necessary to initiate the text model. Can be either:\n",
    "                    - A string, the `model id` of a pretrained model hosted inside a model repo on huggingface.co.\n",
    "                      Valid model ids can be located at the root-level, like ``bert-base-uncased``, or namespaced under\n",
    "                      a user or organization name, like ``dbmdz/bert-base-german-cased``.\n",
    "                    - A path to a `directory` containing model weights saved using\n",
    "                      :func:`~transformers.FlaxPreTrainedModel.save_pretrained`, e.g., ``./my_model_directory/``.\n",
    "                    - A path or url to a `PyTorch checkpoint folder` (e.g, ``./pt_model``). In\n",
    "                      this case, ``from_pt`` should be set to :obj:`True` and a configuration object should be provided\n",
    "                      as ``config`` argument. This loading path is slower than converting the PyTorch checkpoint in\n",
    "                      a Flax model using the provided conversion scripts and loading the Flax model afterwards.\n",
    "            vision_model_name_or_path (:obj: `str`, `optional`, defaults to `None`):\n",
    "                Information necessary to initiate the vision model. Can be either:\n",
    "                    - A string, the `model id` of a pretrained model hosted inside a model repo on huggingface.co.\n",
    "                      Valid model ids can be located at the root-level, like ``bert-base-uncased``, or namespaced under\n",
    "                      a user or organization name, like ``dbmdz/bert-base-german-cased``.\n",
    "                    - A path to a `directory` containing model weights saved using\n",
    "                      :func:`~transformers.FlaxPreTrainedModel.save_pretrained`, e.g., ``./my_model_directory/``.\n",
    "                    - A path or url to a `PyTorch checkpoint folder` (e.g, ``./pt_model``). In\n",
    "                      this case, ``from_pt`` should be set to :obj:`True` and a configuration object should be provided\n",
    "                      as ``config`` argument. This loading path is slower than converting the PyTorch checkpoint in\n",
    "                      a Flax model using the provided conversion scripts and loading the Flax model afterwards.\n",
    "            model_args (remaining positional arguments, `optional`):\n",
    "                All remaning positional arguments will be passed to the underlying model's ``__init__`` method.\n",
    "            kwargs (remaining dictionary of keyword arguments, `optional`):\n",
    "                Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,\n",
    "                :obj:`output_attentions=True`).\n",
    "                - To update the text configuration, use the prefix `text_` for each configuration parameter.\n",
    "                - To update the vision configuration, use the prefix `vision_` for each configuration parameter.\n",
    "                - To update the parent model configuration, do not use a prefix for each configuration parameter.\n",
    "                Behaves differently depending on whether a :obj:`config` is provided or automatically loaded.\n",
    "        Example::\n",
    "            >>> from transformers import FlaxHybridCLIP\n",
    "            >>> # initialize a model from pretrained BERT and CLIP models. Note that the projection layers will be randomly initialized.\n",
    "            >>> # If using CLIP's vision model the vision projection layer will be initialized using pre-trained weights\n",
    "            >>> model = FlaxHybridCLIP.from_text_vision_pretrained('bert-base-uncased', 'openai/clip-vit-base-patch32')\n",
    "            >>> # saving model after fine-tuning\n",
    "            >>> model.save_pretrained(\"./bert-clip\")\n",
    "            >>> # load fine-tuned model\n",
    "            >>> model = FlaxHybridCLIP.from_pretrained(\"./bert-clip\")\n",
    "        \"\"\"\n",
    "\n",
    "        kwargs_text = {\n",
    "            argument[len(\"text_\") :]: value\n",
    "            for argument, value in kwargs.items()\n",
    "            if argument.startswith(\"text_\")\n",
    "        }\n",
    "\n",
    "        kwargs_vision = {\n",
    "            argument[len(\"vision_\") :]: value\n",
    "            for argument, value in kwargs.items()\n",
    "            if argument.startswith(\"vision_\")\n",
    "        }\n",
    "\n",
    "        # remove text, vision kwargs from kwargs\n",
    "        for key in kwargs_text.keys():\n",
    "            del kwargs[\"text_\" + key]\n",
    "        for key in kwargs_vision.keys():\n",
    "            del kwargs[\"vision_\" + key]\n",
    "\n",
    "        # Load and initialize the text and vision model\n",
    "        text_model = kwargs_text.pop(\"model\", None)\n",
    "        if text_model is None:\n",
    "            assert (\n",
    "                text_model_name_or_path is not None\n",
    "            ), \"If `model` is not defined as an argument, a `text_model_name_or_path` has to be defined\"\n",
    "            from transformers import FlaxAutoModel\n",
    "\n",
    "            if \"config\" not in kwargs_text:\n",
    "                from transformers import AutoConfig\n",
    "\n",
    "                text_config = AutoConfig.from_pretrained(text_model_name_or_path)\n",
    "                kwargs_text[\"config\"] = text_config\n",
    "\n",
    "            text_model = FlaxAutoModel.from_pretrained(\n",
    "                text_model_name_or_path, *model_args, **kwargs_text\n",
    "            )\n",
    "\n",
    "        vision_model = kwargs_vision.pop(\"model\", None)\n",
    "        if vision_model is None:\n",
    "            assert (\n",
    "                vision_model_name_or_path is not None\n",
    "            ), \"If `model` is not defined as an argument, a `vision_model_name_or_path` has to be defined\"\n",
    "            from transformers import FlaxAutoModel\n",
    "\n",
    "            if \"config\" not in kwargs_vision:\n",
    "                from transformers import AutoConfig\n",
    "\n",
    "                vision_config = AutoConfig.from_pretrained(vision_model_name_or_path)\n",
    "                kwargs_vision[\"config\"] = vision_config\n",
    "\n",
    "            vision_model = FlaxAutoModel.from_pretrained(\n",
    "                vision_model_name_or_path, *model_args, **kwargs_vision\n",
    "            )\n",
    "\n",
    "        # instantiate config with corresponding kwargs\n",
    "        dtype = kwargs.pop(\"dtype\", jnp.float32)\n",
    "        config = HybridCLIPConfig.from_text_vision_configs(\n",
    "            text_model.config, vision_model.config, **kwargs\n",
    "        )\n",
    "\n",
    "        # init model\n",
    "        model = cls(config, *model_args, dtype=dtype, **kwargs)\n",
    "\n",
    "        if vision_config.model_type == \"clip\":\n",
    "            model.params[\"vision_model\"][\"vision_model\"] = vision_model.params[\n",
    "                \"vision_model\"\n",
    "            ]\n",
    "            model.params[\"visual_projection\"][\"kernel\"] = vision_model.params[\n",
    "                \"visual_projection\"\n",
    "            ][\"kernel\"]\n",
    "        else:\n",
    "            model.params[\"vision_model\"] = vision_model.params\n",
    "\n",
    "        model.params[\"text_model\"] = text_model.params\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from transformers import CLIPProcessor\n",
    "\n",
    "\n",
    "class FlaxHybridCLIPProcessor(CLIPProcessor):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def __call__(self, text=None, images=None, return_tensors=None, **kwargs):\n",
    "        encoding = super().__call__(text, images, return_tensors, **kwargs)\n",
    "        # flax expects channels last\n",
    "        if \"pixel_values\" in encoding.keys():\n",
    "            encoding[\"pixel_values\"] = jnp.transpose(\n",
    "                encoding[\"pixel_values\"], axes=[0, 2, 3, 1]\n",
    "            )\n",
    "        return encoding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Get Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Use KoClip\n",
    "\n",
    "from transformers import AutoTokenizer, ViTFeatureExtractor\n",
    "# from .model import FlaxHybridCLIP\n",
    "# from .processor import FlaxHybridCLIPProcessor\n",
    "\n",
    "\n",
    "def load_koclip(model_name):\n",
    "    assert model_name in {\"koclip-base\", \"koclip-large\"}\n",
    "    model = FlaxHybridCLIP.from_pretrained(f\"koclip/{model_name}\")\n",
    "    processor = FlaxHybridCLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "    processor.tokenizer = AutoTokenizer.from_pretrained(\"klue/roberta-large\")\n",
    "    if model_name == \"koclip/koclip-large\":\n",
    "        processor.feature_extractor = ViTFeatureExtractor.from_pretrained(\n",
    "            \"google/vit-large-patch16-224\"\n",
    "        )\n",
    "    return model, processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, processor = load_koclip(\"koclip-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
